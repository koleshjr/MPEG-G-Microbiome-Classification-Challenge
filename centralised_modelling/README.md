# Centralised Modelling

This module provides scripts and notebooks for training and evaluating machine learning models using the processed feature data generated by the [`data_prep`](../data_prep) module.

## Overview

- **Purpose:** Train and validate models for microbiome classification using rich features extracted from raw sequencing data.
- **Data Source:** All modelling uses the processed CSV files created by the data preparation pipeline in `data_prep/Data/ProcessedFiles/`.
- **Workflow:** Feature selection, cross-validation, and model training are performed in a reproducible, centralised manner.

---

## Directory Structure

```
centralised_modelling/
├── basic_stats_modelling.ipynb   # notebook for feature selection and modelling
├── main.py                      # Main script for running experiments
├── pyproject.toml               # Project dependencies (for uv)
├── uv.lock                      # uv lock file
├── README.md                    # This documentation
```

---

## Data References

This module expects the following processed files from the data preparation step:

- `../data_prep/Data/ProcessedFiles/train_features_with_kmers_new.csv`
- `../data_prep/Data/ProcessedFiles/test_features_with_kmers_new.csv`
- `../data_prep/Data/Train.csv` (original labels)
- `../data_prep/Data/Test.csv` (Test)

Make sure you have run the data preparation pipeline before starting modelling.

---

## Modelling Workflow

- **Feature Selection:** Choose relevant statistical and k-mer features for training.
- **Cross Validation:** Stratified K-Fold cross-validation for robust performance estimation.
- **Model Training:**XGBoost with tuned hyperparameters.
- **Prediction & Submission:** Generates predictions for the test set and saves submission files.

See [`basic_stats_modelling.ipynb`](basic_stats_modelling.ipynb) for a step-by-step example.

---

## Requirements

This module uses [uv](https://github.com/astral-sh/uv) for fast, reproducible Python environments.

### Setup

1. **Install uv (if not already installed):**
   ```bash
   pip install uv
   ```

2. **Create and activate a virtual environment:**
   ```bash
   uv venv
   source .venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   uv sync
   ```

Dependencies are managed in `pyproject.toml`.

---

## Notes

- All modelling code references the processed feature CSVs from the data_prep module.
- You can adapt the modelling scripts for different algorithms or feature sets as needed.
- Outputs (e.g., submission files) are saved in the appropriate directories for easy integration with competition workflows.
- Use the sub saved to reproduce our private score
---
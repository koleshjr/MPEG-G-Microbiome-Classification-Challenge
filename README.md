# MPEG-G Microbiome Classification Challenge

Can you classify microbiome samples by body site and individual using compressed data?

Your goal is to build a machine learning model that classifies a microbiome sample’s body site—stool, oral, skin, or nasal—based on its 16S rRNA gene sequence profile encoded in MPEG-G format, along with health status tags, and participant metadata. Briefly, 16S rRNA is used to identify and classify bacteria by sequencing a specific region of the 16S ribosomal RNA gene and aligning it to a database, which is highly conserved among bacteria, allowing for taxonomic classification.

---

## Project Structure

This repository is organized into three main modules:

- **data_prep/**: Data preparation and feature extraction
- **centralised_modelling/**: Centralised machine learning modelling
- **federated_modelling/**: Federated learning for privacy-preserving distributed modelling

---

## 1. Data Preparation

The `data_prep` module provides a robust pipeline for preparing microbiome sequencing data for machine learning and modeling. It supports both `.mgb` and `.fastq` files, and extracts subtle, high-value features from raw reads using efficient multiprocessing.

### Directory Structure

```
Data/
├── TrainFiles/
│     └── sample.mgb
├── TestFiles/
│     └── sample.mgb
├── TrainFastQ/
│     └── sample.fastq
├── TestFastQ/
│     └── sample.fastq
├── Train.csv
├── Test.csv
```

### Advanced Feature Extraction

- **Multiprocessing:** Efficiently processes large datasets using all available CPU cores.
- **Resumable Processing:** Tracks processed files with log files, allowing safe interruption and restart.
- **Feature Types:**
  - Basic statistics: Number of reads, average read length, GC content, nucleotide fractions (A, T, G, C).
  - Quality metrics: Fractions of bases with Q20 and Q30 phred scores.
  - K-mer features: For k=2,3,4,5, computes normalized frequencies for the most common k-mers (canonical only, configurable).
  - Customizable vocabularies: Builds k-mer vocabularies from training data, ensuring consistent feature sets.

### Usage
0. Git clone the repo
1. Place FASTQ files in the appropriate directories as shown above.
2. Change directory to `data_prep`:
   ```bash
   cd data_prep
   ```
3. Run the script:
   ```bash
   python prepare_data_multiprocessing.py
   ```
4. Outputs:
   - Processed feature CSVs:  
     - `Data/ProcessedFiles/train_features_with_kmers_new.csv`
     - `Data/ProcessedFiles/test_features_with_kmers_new.csv`
   - K-mer vocabulary:  
     - `Data/kmer_vocab_new.json`
   - Logs of processed files:  
     - `Data/train_processed_new.log`
     - `Data/test_processed_new.log`

### Requirements

This module uses [uv](https://github.com/astral-sh/uv) for fast, reproducible Python environments.

```bash
pip install uv
uv venv
source .venv/bin/activate
uv sync
```

### Note
Since this is a time consuming step we have provided all the processed files in the respective subdirectories, so you can just start from Modelling

---

## 2. Centralised Modelling

This module provides scripts and notebooks for training and evaluating machine learning models using the processed feature data generated by the data_prep module.

### Data References

- `../data_prep/Data/ProcessedFiles/train_features_with_kmers_new.csv`
- `../data_prep/Data/ProcessedFiles/test_features_with_kmers_new.csv`
- `../data_prep/Data/Train.csv` (original labels)
- `../data_prep/Data/Test.csv` (test samples)

### Modelling Workflow

- **Feature Selection:** Choose relevant statistical and k-mer features for training.
- **Cross Validation:** Stratified K-Fold cross-validation for robust performance estimation.
- **Model Training:** Example uses XGBoost with tuned hyperparameters.
- **Prediction & Submission:** Generates predictions for the test set and saves submission files.

See [`basic_stats_modelling.ipynb`](centralised_modelling/basic_stats_modelling.ipynb) for a step-by-step example.

### Inference

After training, perform inference using the notebook:

1. Change directory to `centralised_modelling`:
   ```bash
   cd centralised_modelling
   ```
2. Run the notebook:
   ```bash
   jupyter notebook basic_stats_modelling.ipynb
   ```
   or open and run it in your preferred environment.

- This will generate a submission file in `centralised_modelling/Subs/`.

### Requirements

Dependencies are managed in `pyproject.toml`. Use `uv` as above to set up your environment.

---

## 3. Federated Modelling

This module implements federated learning for microbiome classification, enabling distributed training across multiple nodes while preserving data privacy. The workflow is orchestrated using configuration in `pyproject.toml` and automated via the `start_federation.sh` script.

### Data Preparation for Federation
```bash
cd federated_modelling
python data_preparation.py
```

To ensure robust and fair federated learning, we carefully partitioned the data into clients using **Stratified Group K-Fold** based on subject ID. This guarantees:

- **No subject leakage:** Each subject's samples are assigned to only one client, preventing data leakage across clients.
- **Balanced classes:** Each client contains a mix of all sample types (body sites), which is necessary for multi-class logloss to work correctly.

> **Note:**  
> The approach suggested on the competition info page—assigning only one sample type per client—was not feasible, as logloss requires each client to have multiple classes for proper training and evaluation.

The resulting files in `Data/` (e.g., `train_with_5_folds_gkf.csv`) reflect these balanced, non-leaking splits.

---
### Overview

- **Federated Learning:** Multiple clients (nodes) collaboratively train a global model without sharing raw data.
- **Orchestration:** Managed by a central server and multiple clients, with logging and model aggregation.
- **Configuration:** All key parameters (e.g., number of nodes, training settings) are defined in `pyproject.toml`.
- **Inference:** Predictions are made using the final global model produced by the federation.

### Quick Start

1. Change directory to `federated_modelling`:
   ```bash
   cd federated_modelling
   ```
2. **Install Dependencies**
   ```bash
   pip install uv
   uv venv
   source .venv/bin/activate
   uv sync
   ```

3. **Start Federated Learning**
   ```bash
   ./start_federation.sh
   ```
   - Starts the server and all client nodes as defined in your configuration.
   - Prepares data splits for each node.
   - Begins federated training rounds, logging progress in `logs/`.
   - Saves the final global model in `Models/`.

### Inference

After federated training, perform inference using the saved global model:

```bash
python inference.py
```
- Loads the final global model from `Models/` and generates predictions on the test set, saving results in `Subs/`.
- Use the submission file in `Subs/` to reproduce your federated score.

---

## Notes

- Ensure you have run data preparation and have the required CSVs in `Data/`.
- You can customize the federation setup by editing `pyproject.toml` and `start_federation.sh`.
- For advanced usage, modify `client.py`, `server.py`, or `task.py` to implement custom training or aggregation logic.
- All modelling code references the processed feature CSVs from the data_prep module.
- Outputs (e.g., submission files) are saved in the appropriate directories for easy integration with competition workflows.
- **You will get two submission files:** one from centralised modelling and one from federated modelling.

---

## Acknowledgements

This work was a collaborative effort by [Koleshjr](https://github.com/Koleshjr) and [DrCod](https://github.com/DrCod).